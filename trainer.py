# -*- coding: utf-8 -*-
# ---------------------
import logging
import random
from time import time
import numpy as np
import torch
import pandas as pd
from matplotlib import pyplot as plt
from torch import optim
from torch.utils.data import DataLoader
from torch.utils.data._utils.collate import default_collate
from torch.utils.tensorboard import SummaryWriter
from config.config import Config
from dataset.Dataset import Dataset
from temporal_fusion_transformer import TemporalFusionTransformer
from progress_bar import ProgressBar
from utils import QuantileLoss, symmetric_mean_absolute_percentage_error, unnormalize_tensor, plot_temporal_serie, ScaleAwareQuantileLoss
from dataset.traffic_data_formatter import TrafficDataFormatter
from dataset.Dataset_cesnet import CESNETDataset
from utils_modified import combined_quantile_trend_mae_loss
from utils import StrongTrendQuantileLoss


#import data_formatters.utils as utils

class Trainer(object):
    """
    Class for training and test the model
    """

    def __init__(self, cnf):
        # type: (Conf) -> Trainer

        torch.set_num_threads(3)

        # Paths to preprocessed CESNET dataset
        train_path = r"D:\PythonProject\chatbot\dataset\preprocessed\CESNET_100\train.csv"
        val_path = r"D:\PythonProject\chatbot\dataset\preprocessed\CESNET_100\val.csv"
        test_path = r"D:\PythonProject\chatbot\dataset\preprocessed\CESNET_100\test.csv"

        # Set fixed seed for reproducibility
        seed = 42
        torch.manual_seed(seed)
        np.random.seed(seed)
        random.seed(seed)
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

        # Load datasets
        self.dataset_train = CESNETDataset(train_path)
        self.dataset_val = CESNETDataset(val_path)
        self.dataset_test = CESNETDataset(test_path)

        # Load formatter for inverse transform
        self.formatter = TrafficDataFormatter(
            scaler_path=r"D:\PythonProject\chatbot\dataset\preprocessed\CESNET_100\scaler.save")

        print(f"Training dataset shape: {self.dataset_train.inputs.shape}")
        print(f"Validation dataset shape: {self.dataset_val.inputs.shape}")
        print(f"Test dataset shape: {self.dataset_test.inputs.shape}")

        self.cnf = cnf
        input_dim = self.dataset_train.inputs.shape[-1]
        self.cnf.all_params["input_size"] = input_dim
        self.cnf.all_params["input_obs_loc"] = self.dataset_train.input_obs_loc

        device = torch.device("cuda")
        self.cnf.device = "cuda"

        self.model = TemporalFusionTransformer(self.cnf.all_params).to(device)

        # Use strong trend quantile loss
        self.loss = StrongTrendQuantileLoss(cnf.quantiles, low_penalty=20.0, trend_weight=1.0, offset_weight=0.3)

        collate_fn = self.cesnet_collate_fn

        self.train_loader = DataLoader(
            dataset=self.dataset_train, batch_size=cnf.batch_size,
            num_workers=cnf.n_workers, shuffle=True, pin_memory=False, drop_last=True, collate_fn=collate_fn
        )
        self.val_loader = DataLoader(
            self.dataset_val, batch_size=cnf.batch_size, shuffle=False,
            num_workers=cnf.n_workers, pin_memory=True, drop_last=False
        )
        self.test_loader = DataLoader(
            dataset=self.dataset_test, batch_size=cnf.batch_size,
            num_workers=cnf.n_workers, shuffle=False, pin_memory=False, drop_last=False
        )

        self.log_path = cnf.exp_log_path
        print(f'tensorboard --logdir={cnf.project_log_path.resolve()}\n')

        # Initialize optimizer and learning rate scheduler
        self.optimizer = optim.Adam(params=self.model.parameters(), lr=cnf.lr, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=0.002,
            total_steps=self.cnf.epochs * len(self.train_loader),
            pct_start=0.3
        )

        self.sw = SummaryWriter(self.log_path)
        self.log_freq = len(self.train_loader)
        self.train_losses = []
        self.test_loss = []
        self.test_losses = {'p10': [], 'p50': [], 'p90': []}
        self.test_smape = []
        self.filtered_smape = []
        self.test_mse = []
        self.test_r2 = []
        self.test_pearson = []

        self.log_file = self.log_path / "training_log.txt"

        logging.basicConfig(
            filename=self.log_file,
            filemode="a",
            format="%(asctime)s - %(levelname)s - %(message)s",
            level=logging.INFO
        )

        self.epoch = 0
        self.best_test_loss = None

        self.progress_bar = ProgressBar(max_step=self.log_freq, max_epoch=self.cnf.epochs)

        self.load_ck()

        print("Finished preparing datasets.")

    @staticmethod
    def cesnet_collate_fn(batch):
        # Filter out None samples
        batch = [sample for sample in batch if sample is not None]
        if not batch:
            # If all samples are None, return a default empty batch
            return {
                'inputs': torch.zeros((0, 96, 41), dtype=torch.float32),
                'outputs': torch.zeros((0, 48, 1), dtype=torch.float32),
                'time_idx': torch.zeros((0, 48, 1), dtype=torch.int64),
                'group_ids': torch.zeros((0, 1), dtype=torch.int64),
            }

        # Standard collation for valid samples
        inputs = torch.stack([sample['inputs'] for sample in batch])
        outputs = torch.stack([sample['outputs'] for sample in batch])
        time_idx = torch.stack([sample['time_idx'] for sample in batch])
        group_ids = torch.stack([sample['group_ids'] for sample in batch])

        return {
            'inputs': inputs,
            'outputs': outputs,
            'time_idx': time_idx,
            'group_ids': group_ids,
        }

    def load_ck(self):
        """
        load training checkpoint
        """
        ck_path = self.log_path / 'training.ck'
        if ck_path.exists():
            #ck = torch.load(ck_path)
            ck = torch.load(ck_path, map_location="cuda")
            print(f'[loading checkpoint \'{ck_path}\']')
            self.epoch = ck['epoch']
            self.progress_bar.current_epoch = self.epoch
            self.model.load_state_dict(ck['model'])
            self.optimizer.load_state_dict(ck['optimizer'])
            #self.best_test_loss = self.best_test_loss
            self.best_test_loss = ck.get("best_test_loss", None)


    def save_ck(self):
        """
        save training checkpoint
        """

        ck = {
            'epoch': self.epoch,
            'model': self.model.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'best_test_loss': self.best_test_loss
        }

        import os
        import glob

        # Ensure log path exists
        os.makedirs(self.log_path, exist_ok=True)

        # Save current epoch checkpoint
        checkpoint_path = os.path.join(self.log_path, f'epoch_{self.epoch}.pth')
        torch.save(ck, checkpoint_path)

        # Manage checkpoint files - keep only the latest 10 epoch checkpoints
        epoch_files = sorted(glob.glob(os.path.join(self.log_path, 'epoch_*.pth')))

        # # Remove oldest checkpoints if more than 10
        # while len(epoch_files) > 10:
        #     oldest_file = epoch_files.pop(0)
        #     os.remove(oldest_file)
        #     print(f"ğŸ—‘ï¸ Removed old checkpoint: {oldest_file}")

        if self.best_test_loss is None or ck["best_test_loss"] < self.best_test_loss:
            self.best_test_loss = ck["best_test_loss"]
            # torch.save(self.model.state_dict(), self.log_path / self.cnf.exp_name + '_best.pth')
            import os
            torch.save(self.model.state_dict(), os.path.join(self.log_path, f"{self.cnf.exp_name}_best.pth"))

    @staticmethod
    def unnormalize_tensor(target, tensor):
        scale = target.max() - target.min()
        mean = target.mean()
        return tensor * scale + mean

    # Add this method in the Trainer class
    def calculate_regularized_loss(self, y_pred, y_true, attention_components=None):
        """
        Compute regularized loss combining quantile loss, variance encouragement, and entropy regularization.

        Args:
            y_pred (Tensor): Model predictions with quantile outputs [batch, time, quantiles]
            y_true (Tensor): Ground truth values [batch, time]
            attention_components (dict): Optional attention weight outputs from the model

        Returns:
            tuple: (total_loss, quantile_loss_only)
        """
        # 1. Compute base quantile loss
        quantile_loss, _ = self.loss(y_pred, y_true)

        # 2. Add variance encouragement term
        if y_pred.dim() == 3:  # [batch, time, quantiles]
            p50_pred = y_pred[:, :, 1]  # Use p50 (median) prediction
        else:
            p50_pred = y_pred

        # Encourage diversity in predictions by rewarding higher variance
        pred_variance = torch.var(p50_pred, dim=1).mean()
        var_weight = 0.05  # Weight for variance reward
        var_term = -var_weight * torch.log(pred_variance + 1e-6)

        # 3. Add entropy regularization on attention weights
        entropy_loss = 0.0
        entropy_weight = 0.1  # Entropy regularization weight

        if attention_components is not None:
            # Handle historical attention weights
            if 'historical_flags' in attention_components:
                hist_weights = attention_components['historical_flags']
                if hist_weights.dim() > 2:
                    hist_weights = hist_weights.squeeze(2)  # Remove singleton dimension
                hist_weights = torch.clamp(hist_weights, min=1e-8)
                hist_entropy = -torch.sum(hist_weights * torch.log(hist_weights), dim=-1)
                entropy_loss -= entropy_weight * hist_entropy.mean()

            # Handle future attention weights
            if 'future_flags' in attention_components:
                future_weights = attention_components['future_flags']
                if future_weights.dim() > 2:
                    future_weights = future_weights.squeeze(2)
                future_weights = torch.clamp(future_weights, min=1e-8)
                future_entropy = -torch.sum(future_weights * torch.log(future_weights), dim=-1)
                entropy_loss -= entropy_weight * future_entropy.mean()

        # 4. Combine all components
        total_loss = quantile_loss + var_term + entropy_loss

        return total_loss, quantile_loss  # Return total and raw quantile loss

    def train(self):
        """
        Perform one epoch of model training.
        """
        start_time = time()
        self.model.train()

        times = []

        for step, sample in enumerate(self.train_loader):
            t = time()
            self.optimizer.zero_grad()
            try:
                x = sample['inputs'].float().to("cuda")
                output, _, attention_components = self.model.forward(x)
            except Exception as e:
                print(f"Training error: {e}")
                raise
            loss = combined_quantile_trend_mae_loss(output, sample['outputs'].squeeze().float().to("cuda"))

            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cnf.all_params['max_gradient_norm'])
            self.train_losses.append(loss.item())
            self.optimizer.step()
            self.scheduler.step()

            # Print progress bar
            times.append(time() - t)
            if self.cnf.log_each_step or (not self.cnf.log_each_step and self.progress_bar.progress == 1):
                print(f'\r{self.progress_bar} '
                      f'â”‚ Loss: {np.mean(self.train_losses):.6f} '
                      f'â”‚ â†¯: {1 / np.mean(times):5.2f} step/s', end='')
            self.progress_bar.inc()

        # Log average loss for this epoch
        mean_epoch_loss = np.mean(self.train_losses)
        logging.info(f"Epoch {self.epoch}: Train Loss: {mean_epoch_loss:.6f}")
        self.sw.add_scalar(tag='train_loss', scalar_value=mean_epoch_loss, global_step=self.epoch)
        self.train_losses = []

        # Log total time for this epoch
        print(f' â”‚ T: {time() - start_time:.2f} s')

    def inverse_transform_single_feature(self, scaler, values, feature_idx=0):
        """
        Apply inverse transformation to a single feature using a fitted scaler.

        Args:
            scaler: A fitted scaler object with inverse_transform method
            values: The normalized values to inverse transform, shape (n_samples, 1)
            feature_idx: Index of the target feature in the original input space

        Returns:
            numpy.ndarray: The de-normalized values with the same shape as input
        """
        # Create a zero matrix with the same feature dimension as expected by the scaler
        n_features = len(scaler.center_)
        dummy = np.zeros((len(values), n_features))

        # Fill in the target feature at the specified index
        dummy[:, feature_idx] = values.flatten()

        # Apply inverse transform
        transformed = scaler.inverse_transform(dummy)

        # Return only the target feature
        return transformed[:, feature_idx].reshape(values.shape)

    def nonlinear_correction(self, values, base=0.2, high_scale=1.4, threshold=0.5):
        # æ³¨æ„è¿™é‡Œç¬¬ä¸€ä¸ªå‚æ•°æ˜¯self
        corrected = np.copy(values)
        low_mask = values < threshold
        high_mask = values >= threshold

        # å¯¹ä½å€¼åŒºåŸŸè½»å¾®æå‡
        corrected[low_mask] = values[low_mask] + base

        # å¯¹é«˜å€¼åŒºåŸŸæ›´å¼ºæ ¡æ­£
        corrected[high_mask] = base + values[high_mask] * high_scale

        return corrected

    def test(self):
        """
        Test the model on the test set and generate visualizations.
        """
        # Log test dataset information
        print(f"Test dataset size: {len(self.dataset_test)}")
        print(f"Number of test batches: {len(self.val_loader)}")
        print(f"Batch size: {self.cnf.batch_size}")

        self.model.eval()

        # Import visualization libraries
        import matplotlib.pyplot as plt
        import matplotlib
        matplotlib.use('Agg')  # Use non-interactive backend for saving plots

        # Initialize containers for evaluation metrics and predictions
        all_outputs = []
        all_targets = []
        all_timestamps = []  # Store timestamps if available

        self.test_losses = {'p10': [], 'p50': [], 'p90': []}
        self.test_loss = []
        self.test_smape = []
        self.test_mse = []
        self.test_r2 = []
        self.test_pearson = []
        self.filtered_smape = []

        # Store predictions and targets for visualization
        sample_predictions = []
        sample_targets = []
        sample_times = []

        t = time()

        with torch.no_grad():
            for step, sample in enumerate(self.val_loader):
                target_values = None
                p10_values = None
                p50_values = None
                p90_values = None
                # åœ¨æ¯ä¸ªæ‰¹æ¬¡å¤„ç†å¼€å§‹æ—¶
                # print(f"å¤„ç†æ‰¹æ¬¡ {step}, æ ·æœ¬æ•°é‡: {x.shape[0]}")
                if sample is None:
                    print(f"âš ï¸ Skipping empty batch at step {step}")
                    continue
                device = "cuda"

                self.model = self.model.to(device)

                steps = self.cnf.all_params['num_encoder_steps']
                pred_len = sample['outputs'].shape[1]
                x = sample['inputs'].float().to(device)
                x[:, steps:, 0] = 1

                output, _, attention_components = self.model.forward(x)

                print(f"åŸå§‹è¾“å‡ºå½¢çŠ¶: {output.shape}")

                # æ ¹æ®è¾“å‡ºçš„å®é™…ç»´åº¦ï¼Œé€‚å½“åœ°è¿›è¡Œsqueeze
                if output.dim() == 4:  # [batch, time, 1, quantiles]
                    output = output.squeeze(2)
                elif output.dim() == 3:  # [batch, time, quantiles]
                    output = output
                else:
                    print(f"è­¦å‘Šï¼šè¾“å‡ºç»´åº¦æ„å¤–: {output.dim()}")

                print(f"å¤„ç†åè¾“å‡ºå½¢çŠ¶: {output.shape}")

                # è·å–ç›®æ ‡å€¼ï¼Œæ‰“å°å½¢çŠ¶
                y = sample['outputs'].to(device)
                print(f"åŸå§‹ç›®æ ‡å½¢çŠ¶: {y.shape}")

                # é€‚å½“è°ƒæ•´ç›®æ ‡å€¼å½¢çŠ¶
                if y.dim() == 3:  # [batch, time, 1]
                    y_reshaped = y.squeeze(-1)
                else:
                    y_reshaped = y

                print(f"è°ƒæ•´åç›®æ ‡å½¢çŠ¶: {y_reshaped.shape}")

                # è®¡ç®—æŸå¤±
                # loss, _ = combined_quantile_trend_mae_loss(output, y_reshaped)
                loss = combined_quantile_trend_mae_loss(output, y_reshaped)

                # è½¬æ¢ä¸ºnumpyè¿›è¡ŒæŒ‡æ ‡è®¡ç®—ï¼Œæ‰“å°æ¯ä¸ªè½¬æ¢åçš„å½¢çŠ¶
                output_np = output.detach().cpu().numpy()
                y_np = y_reshaped.detach().cpu().numpy()

                print(f"output_npå½¢çŠ¶: {output_np.shape}")
                print(f"y_npå½¢çŠ¶: {y_np.shape}")

                # å¦‚æœæ ·æœ¬ä¸­æœ‰æ—¶é—´æˆ³ä¿¡æ¯ï¼Œä¿å­˜å®ƒ
                if 'timestamps' in sample:
                    timestamps = sample['timestamps'].detach().cpu().numpy()
                    all_timestamps.append(timestamps)
                    # ä¿å­˜ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„æ—¶é—´æˆ³ï¼Œç”¨äºå¯è§†åŒ–
                    if step == 0:
                        sample_times = timestamps[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ—¶é—´æˆ³
                else:
                    # å¦‚æœæ²¡æœ‰æ—¶é—´æˆ³ï¼Œä½¿ç”¨åºåˆ—ç´¢å¼•
                    batch_size = y_np.shape[0]
                    seq_length = y_np.shape[1] if y_np.ndim > 1 else 1
                    timestamps = np.arange(seq_length)
                    # ä¿å­˜ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„æ—¶é—´æˆ³ï¼Œç”¨äºå¯è§†åŒ–
                    if step == 0:
                        sample_times = timestamps

                # æ ¹æ®å®é™…ç»´åº¦è¿›è¡Œç´¢å¼•ï¼Œé¿å…ç»´åº¦ä¸åŒ¹é…é”™è¯¯
                if output_np.ndim == 3 and output_np.shape[2] >= 3:  # [batch, time, quantiles]
                    p10_values = output_np[:, :, 0]
                    p50_values = output_np[:, :, 1]
                    p90_values = output_np[:, :, 2]

                if sample is None:  # å¦‚æœbatchä¸ºç©ºï¼Œè·³è¿‡
                    print(f"âš ï¸ Skipping empty batch at step {step}")
                    continue

                print(f"å¤„ç†æ‰¹æ¬¡ {step}, æ ·æœ¬æ•°é‡: {sample['inputs'].shape[0]}")

                # é€‰æ‹©è®¾å¤‡ - æ­¤å¤„å›ºå®šä½¿ç”¨cuda
                device = "cuda"  # æˆ–è€… "cpu"

                # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                self.model = self.model.to(device)

                # è®¾ç½®è¾“å…¥æ•°æ®
                steps = self.cnf.all_params['num_encoder_steps']
                pred_len = sample['outputs'].shape[1]
                x = sample['inputs'].float().to(device)
                x[:, steps:, 0] = 1

                # å‰å‘ä¼ æ’­
                output, _, attention_components = self.model.forward(x)

                # æ‰“å°åŸå§‹è¾“å‡ºå½¢çŠ¶ï¼Œå¸®åŠ©è°ƒè¯•
                print(f"åŸå§‹è¾“å‡ºå½¢çŠ¶: {output.shape}")

                # æ ¹æ®è¾“å‡ºçš„å®é™…ç»´åº¦ï¼Œé€‚å½“åœ°è¿›è¡Œsqueeze
                if output.dim() == 4:  # [batch, time, 1, quantiles]
                    output = output.squeeze(2)
                elif output.dim() == 3:  # [batch, time, quantiles]
                    output = output
                else:
                    print(f"è­¦å‘Šï¼šè¾“å‡ºç»´åº¦æ„å¤–: {output.dim()}")

                print(f"å¤„ç†åè¾“å‡ºå½¢çŠ¶: {output.shape}")

                # è·å–ç›®æ ‡å€¼ï¼Œæ‰“å°å½¢çŠ¶
                y = sample['outputs'].to(device)
                print(f"åŸå§‹ç›®æ ‡å½¢çŠ¶: {y.shape}")

                # é€‚å½“è°ƒæ•´ç›®æ ‡å€¼å½¢çŠ¶
                if y.dim() == 3:  # [batch, time, 1]
                    y_reshaped = y.squeeze(-1)
                else:
                    y_reshaped = y

                print(f"è°ƒæ•´åç›®æ ‡å½¢çŠ¶: {y_reshaped.shape}")

                # è®¡ç®—æŸå¤±
                # loss, _ = self.loss(output, y_reshaped)
                loss = combined_quantile_trend_mae_loss(output, y_reshaped)

                # è½¬æ¢ä¸ºnumpyè¿›è¡ŒæŒ‡æ ‡è®¡ç®—ï¼Œæ‰“å°æ¯ä¸ªè½¬æ¢åçš„å½¢çŠ¶
                output_np = output.detach().cpu().numpy()
                y_np = y_reshaped.detach().cpu().numpy()

                print(f"output_npå½¢çŠ¶: {output_np.shape}")
                print(f"y_npå½¢çŠ¶: {y_np.shape}")

                # å¦‚æœæ ·æœ¬ä¸­æœ‰æ—¶é—´æˆ³ä¿¡æ¯ï¼Œä¿å­˜å®ƒ
                if 'timestamps' in sample:
                    timestamps = sample['timestamps'].detach().cpu().numpy()
                    all_timestamps.append(timestamps)
                    # ä¿å­˜ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„æ—¶é—´æˆ³ï¼Œç”¨äºå¯è§†åŒ–
                    if step == 0:
                        sample_times = timestamps[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ—¶é—´æˆ³
                else:
                    # å¦‚æœæ²¡æœ‰æ—¶é—´æˆ³ï¼Œä½¿ç”¨åºåˆ—ç´¢å¼•
                    batch_size = y_np.shape[0]
                    seq_length = y_np.shape[1] if y_np.ndim > 1 else 1
                    timestamps = np.arange(seq_length)
                    # ä¿å­˜ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„æ—¶é—´æˆ³ï¼Œç”¨äºå¯è§†åŒ–
                    if step == 0:
                        sample_times = timestamps

                # æ ¹æ®å®é™…ç»´åº¦è¿›è¡Œç´¢å¼•ï¼Œé¿å…ç»´åº¦ä¸åŒ¹é…é”™è¯¯
                if output_np.ndim == 3 and output_np.shape[2] >= 3:  # [batch, time, quantiles]
                    p10_values = output_np[:, :, 0]
                    p50_values = output_np[:, :, 1]
                    p90_values = output_np[:, :, 2]

                    # ä¿å­˜ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„å‰å‡ ä¸ªæ ·æœ¬ï¼Œç”¨äºå¯è§†åŒ–
                    if step == 0:
                        for i in range(min(3, output_np.shape[0])):  # ä¿å­˜æœ€å¤š3ä¸ªæ ·æœ¬
                            sample_predictions.append({
                                'p10': p10_values[i],
                                'p50': p50_values[i],
                                'p90': p90_values[i]
                            })

                elif output_np.ndim == 2:  # [batch, quantiles] æˆ– [batch*time, quantiles]
                    # å‡è®¾è¿™ç§æƒ…å†µä¸‹æ¯è¡Œæ˜¯[p10, p50, p90]
                    if output_np.shape[1] >= 3:
                        p10_values = output_np[:, 0]
                        p50_values = output_np[:, 1]
                        p90_values = output_np[:, 2]

                        # é‡å¡‘ä¸º[batch, time]æ ¼å¼ï¼Œå¦‚æœåŸæœ¬æ˜¯å±•å¹³çš„
                        batch_size = y_np.shape[0]
                        if y_np.ndim > 1:
                            seq_length = y_np.shape[1]
                            if len(p50_values) == batch_size * seq_length:
                                p10_values = p10_values.reshape(batch_size, seq_length)
                                p50_values = p50_values.reshape(batch_size, seq_length)
                                p90_values = p90_values.reshape(batch_size, seq_length)

                        # ä¿å­˜ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„å‰å‡ ä¸ªæ ·æœ¬ï¼Œç”¨äºå¯è§†åŒ–
                        if step == 0:
                            for i in range(min(3, batch_size)):  # ä¿å­˜æœ€å¤š3ä¸ªæ ·æœ¬
                                if p50_values.ndim > 1:
                                    sample_predictions.append({
                                        'p10': p10_values[i],
                                        'p50': p50_values[i],
                                        'p90': p90_values[i]
                                    })
                                else:
                                    # å¦‚æœæ˜¯1Dæ•°ç»„ï¼Œåˆ™æ•´ä¸ªæ•°ç»„ä½œä¸ºä¸€ä¸ªæ ·æœ¬
                                    sample_predictions.append({
                                        'p10': p10_values,
                                        'p50': p50_values,
                                        'p90': p90_values
                                    })
                                    break  # åªä¿å­˜ä¸€ä¸ª

                    else:
                        print(f"è­¦å‘Š: è¾“å‡ºå½¢çŠ¶ä¸åŒ…å«è¶³å¤Ÿçš„åˆ†ä½æ•°: {output_np.shape}")
                        # å‡è®¾åªæœ‰ä¸€ä¸ªåˆ†ä½æ•°
                        p10_values = p50_values = p90_values = output_np.flatten()

                        if step == 0:
                            sample_predictions.append({
                                'p10': p10_values,
                                'p50': p50_values,
                                'p90': p90_values
                            })
                else:
                    print(f"è­¦å‘Š: æ„å¤–çš„è¾“å‡ºå½¢çŠ¶: {output_np.shape}")
                    # ä½¿ç”¨flattenæ¥åº”å¯¹ä¸æ˜ç¡®çš„å½¢çŠ¶
                    p10_values = p50_values = p90_values = output_np.flatten()

                    if step == 0:
                        sample_predictions.append({
                            'p10': p10_values,
                            'p50': p50_values,
                            'p90': p90_values
                        })
                # # åº”ç”¨æ ¡æ­£
                # p10_values = self.nonlinear_correction(p10_values)
                # p50_values = self.nonlinear_correction(p50_values)
                # p90_values = self.nonlinear_correction(p90_values)

                # åŒæ ·ï¼Œæ ¹æ®å®é™…ç»´åº¦è·å–ç›®æ ‡å€¼ - ç¡®ä¿æ¯ä¸ªæ‰¹æ¬¡éƒ½é‡æ–°è·å–
                if y_np.ndim == 3:  # [batch, time, 1]
                    target_values = y_np[:, :, 0]
                    # ä¿å­˜ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„å‰å‡ ä¸ªæ ·æœ¬çš„ç›®æ ‡ï¼Œç”¨äºå¯è§†åŒ–
                    if step == 0:
                        for i in range(min(3, y_np.shape[0])):
                            sample_targets.append(target_values[i])
                elif y_np.ndim == 2:  # [batch, time]
                    target_values = y_np
                    # ä¿å­˜ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„å‰å‡ ä¸ªæ ·æœ¬çš„ç›®æ ‡ï¼Œç”¨äºå¯è§†åŒ–
                    if step == 0:
                        for i in range(min(3, y_np.shape[0])):
                            sample_targets.append(target_values[i])
                else:
                    target_values = y_np.flatten()
                    if step == 0:
                        sample_targets.append(target_values)

                print(f"p50_valueså½¢çŠ¶: {p50_values.shape}")
                print(f"target_valueså½¢çŠ¶: {target_values.shape}")
                print(f"[DEBUG] p50 mean: {p50_values.mean():.4f}, target mean: {target_values.mean():.4f}")
                # æ‰“å°åŸå§‹å€¼èŒƒå›´
                # print(f"åº”ç”¨åç§»å‰ p50 min: {np.min(p50_values):.3f}, max: {np.max(p50_values):.3f}")
                #
                # # åº”ç”¨åç§»
                # offset = 0.4  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
                # p10_values += offset
                # p50_values += offset
                # p90_values += offset

                # # æ‰“å°åº”ç”¨åç§»åçš„å€¼èŒƒå›´
                # print(f"åº”ç”¨åç§»å p50 min: {np.min(p50_values):.3f}, max: {np.max(p50_values):.3f}")
                # è®¡ç®—SMAPEï¼Œç¡®ä¿å½¢çŠ¶ä¸€è‡´
                if p50_values.shape == target_values.shape:
                    smape = symmetric_mean_absolute_percentage_error(p50_values, target_values)
                else:
                    print(
                        f"è­¦å‘Š: é¢„æµ‹å’Œç›®æ ‡å½¢çŠ¶ä¸åŒ¹é… - p50_values:{p50_values.shape}, target_values:{target_values.shape}")
                    # å°è¯•é‡å¡‘åˆ°ä¸€ç»´å¹¶æˆªå–ç›¸åŒé•¿åº¦
                    p50_flat = p50_values.flatten()
                    target_flat = target_values.flatten()
                    min_length = min(len(p50_flat), len(target_flat))
                    p50_flat = p50_flat[:min_length]
                    target_flat = target_flat[:min_length]
                    smape = symmetric_mean_absolute_percentage_error(p50_flat, target_flat)
                    print(f"ä½¿ç”¨æˆªæ–­åçš„ä¸€ç»´æ•°ç»„è®¡ç®—SMAPE: é•¿åº¦={min_length}")

                print(f"True Min: {np.min(target_values)}, Max: {np.max(target_values)}")
                print(f"Pred Min: {np.min(p50_values)}, Max: {np.max(p50_values)}")

                # ç¡®ä¿æ‰€æœ‰æ•°æ®æ˜¯ä¸€ç»´çš„ä»¥è¿›è¡ŒæŒ‡æ ‡è®¡ç®—
                p10 = p10_values.flatten()
                p50 = p50_values.flatten()
                p90 = p90_values.flatten()
                target = target_values.flatten()

                # å­˜å‚¨å½“å‰æ‰¹æ¬¡çš„ç»“æœï¼Œç”¨äºæ•´ä½“æŒ‡æ ‡è®¡ç®—
                all_outputs.append(p50)
                all_targets.append(target)

                # MSEè®¡ç®—
                def mse(y_true, y_pred):
                    return np.mean((y_true - y_pred) ** 2)

                # å¦‚æœæ•°æ®ä¸­æœ‰æç«¯å€¼ï¼Œä½¿ç”¨å®‰å…¨çš„æœ€å¤§å€¼
                safe_max_p50 = np.nanmax(p50) if np.nanmax(p50) > 0 else 1.0
                safe_max_target = np.nanmax(target) if np.nanmax(target) > 0 else 1.0

                # å½’ä¸€åŒ–åè®¡ç®—MSE
                mse_val = mse(p50 / safe_max_p50, target / safe_max_target)

                # è®¡ç®—Pearsonç›¸å…³ç³»æ•°
                def pearson_corr(y_true, y_pred):
                    # å¤„ç†NaNå’ŒInf
                    y_true = np.nan_to_num(y_true, nan=0.0, posinf=0.0, neginf=0.0)
                    y_pred = np.nan_to_num(y_pred, nan=0.0, posinf=0.0, neginf=0.0)

                    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„éé›¶å€¼è®¡ç®—ç›¸å…³æ€§
                    if np.std(y_true) == 0 or np.std(y_pred) == 0:
                        return 0.0

                    return np.corrcoef(y_true, y_pred)[0, 1]

                pearson = pearson_corr(p50, target)

                # è¿‡æ»¤éé›¶å€¼è¿›è¡Œè¯„ä¼°
                valid_idx = (target > 0)
                if np.sum(valid_idx) > 0:  # ç¡®ä¿æœ‰æœ‰æ•ˆå€¼
                    filtered_target = target[valid_idx]
                    filtered_p50 = p50[valid_idx]

                    filtered_smape = symmetric_mean_absolute_percentage_error(filtered_target, filtered_p50)

                    from sklearn.metrics import r2_score
                    # ç¡®ä¿æ²¡æœ‰NaNæˆ–Inf
                    filtered_target = np.nan_to_num(filtered_target, nan=0.0)
                    filtered_p50 = np.nan_to_num(filtered_p50, nan=0.0)

                    r2_test = r2_score(filtered_target, filtered_p50)
                else:
                    filtered_smape = 0.0
                    r2_test = 0.0

                # è®¡ç®—åˆ†ä½æ•°æŸå¤±
                self.test_losses['p10'].append(self.loss.numpy_normalised_quantile_loss(p10, target, 0.1))
                self.test_losses['p50'].append(self.loss.numpy_normalised_quantile_loss(p50, target, 0.5))
                self.test_losses['p90'].append(self.loss.numpy_normalised_quantile_loss(p90, target, 0.9))

                # ä¿å­˜å…¶ä»–æŒ‡æ ‡
                self.test_loss.append(loss.item())
                self.test_smape.append(smape)
                self.test_mse.append(mse_val)
                self.test_r2.append(r2_test)
                self.test_pearson.append(pearson)
                self.filtered_smape.append(filtered_smape)

                # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼Œç”Ÿæˆæ³¨æ„åŠ›çƒ­å›¾
                if step == 0 and attention_components is not None:
                    self._plot_attention_maps(attention_components)

            # æ¯10ä¸ªepochæˆ–æœ€åä¸€ä¸ªepochæ—¶åˆ›å»ºé¢„æµ‹å¯è§†åŒ–å›¾è¡¨
        if (sample_predictions and sample_targets) and (
                self.epoch % 1 == 0 or self.epoch == self.cnf.all_params['epochs'] - 1):
            # self._plot_predictions(sample_predictions, sample_targets, sample_times)
            self._plot_predictions(sample_predictions, sample_targets, sample_times, sample_indices=[0, 1, 2])

            # æ¯10ä¸ªepochæˆ–æœ€åä¸€ä¸ªepochæ—¶åˆ›å»ºæ•´ä½“è¯¯å·®åˆ†å¸ƒå›¾
        if (self.epoch % 1 == 0 or self.epoch == self.cnf.all_params['epochs'] - 1):
            self._plot_error_distribution(all_outputs, all_targets)

        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœç”¨äºæ•´ä½“ç›¸å…³æ€§åˆ†æ
        all_outputs = np.concatenate(all_outputs) if all_outputs else np.array([])
        all_targets = np.concatenate(all_targets) if all_targets else np.array([])

        # åˆ›å»ºé¢„æµ‹vså®é™…å€¼æ•£ç‚¹å›¾
        # æ¯10ä¸ªepochæˆ–æœ€åä¸€ä¸ªepochæ—¶åˆ›å»ºé¢„æµ‹vså®é™…å€¼æ•£ç‚¹å›¾
        if (self.epoch % 1 == 0 or self.epoch == self.cnf.all_params['epochs'] - 1):
            self._plot_scatter(all_outputs, all_targets)

        # è®°å½•åˆ†ä½æ•°æŸå¤±
        for k in self.test_losses.keys():
            mean_test_loss = np.mean(self.test_losses[k])
            print(f'\tâ— AVG {k} Loss on TEST-set: {mean_test_loss:.6f} â”‚ T: {time() - t:.2f} s')
            self.sw.add_scalar(tag=k + '_test_loss', scalar_value=mean_test_loss, global_step=self.epoch)

        # è®¡ç®—å¹¶è®°å½•æ•´ä½“æŒ‡æ ‡
        mean_test_loss = np.mean(self.test_loss)
        mean_smape = np.mean(self.test_smape)
        mean_mse = np.mean(self.test_mse)
        mean_r2 = np.mean(self.test_r2)
        mean_pearson = np.mean(self.test_pearson)
        mean_filtered_smape = np.mean(self.filtered_smape)

        print(f'\tâ— AVG Loss on TEST-set: {mean_test_loss:.6f} â”‚ T: {time() - t:.2f} s')
        print(f'\tâ— AVG SMAPE on TEST-set: {mean_smape:.6f} â”‚ T: {time() - t:.2f} s')
        print(f'\tâ— AVG MSE on TEST-set: {mean_mse:.6f} â”‚ T: {time() - t:.2f} s')
        print(f'\tâ— AVG r2_score on TEST-set: {mean_r2:.6f} â”‚ T: {time() - t:.2f} s')
        print(f'\tâ— AVG PEARSON on TEST-set: {mean_pearson:.6f} â”‚ T: {time() - t:.2f} s')
        print(f'\tâ— FILTERED SMAPE on TEST-set: {mean_filtered_smape:.6f} â”‚ T: {time() - t:.2f} s')

        # è®°å½•åˆ°TensorBoard
        self.sw.add_scalar(tag='test_smape', scalar_value=mean_smape, global_step=self.epoch)
        self.sw.add_scalar(tag='filtered_smape', scalar_value=mean_filtered_smape, global_step=self.epoch)
        self.sw.add_scalar(tag='test_loss', scalar_value=mean_test_loss, global_step=self.epoch)

        # è®°å½•åˆ°æ—¥å¿—
        logging.info(
            f"Epoch {self.epoch}: Test Loss: {mean_test_loss:.6f}, SMAPE: {mean_smape:.6f}, filtered_smape: {mean_filtered_smape:.6f}, MSE:{mean_mse:.6f}, R2:{mean_r2:.6f}, PEARSON:{mean_pearson:.6f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if self.best_test_loss is None or mean_test_loss < self.best_test_loss:
            self.best_test_loss = mean_test_loss
            import os
            torch.save(self.model.state_dict(), os.path.join(self.log_path, f"{self.cnf.exp_name}_best.pth"))

    def _plot_predictions(self, predictions, targets, times, sample_indices=None):
        """
        ç»˜åˆ¶å¤šä¸ªé¢„æµ‹ç»“æœä¸çœŸå®å€¼çš„å¯¹æ¯”å›¾ï¼Œæ”¯æŒæŒ‡å®šç‰¹å®šæ ·æœ¬ç´¢å¼•

        Args:
            predictions: é¢„æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«p10, p50, p90ä¸‰ä¸ªåˆ†ä½æ•°
            targets: çœŸå®å€¼åˆ—è¡¨
            times: æ—¶é—´æˆ³æˆ–ç´¢å¼•
            sample_indices: è¦ç»˜åˆ¶çš„æ ·æœ¬ç´¢å¼•åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºNone
        """
        import matplotlib.pyplot as plt
        import os
        import numpy as np

        # ç¡®ä¿ä¿å­˜å›¾è¡¨çš„ç›®å½•å­˜åœ¨
        plot_dir = os.path.join(self.log_path, 'plots')
        if not os.path.exists(plot_dir):
            os.makedirs(plot_dir)

        # å¦‚æœæ²¡æœ‰æŒ‡å®šsample_indicesï¼Œä½¿ç”¨0, 1, 2
        if sample_indices is None:
            sample_indices = [0, 1, 2]

        # é™åˆ¶æ ·æœ¬æ•°é‡ä¸è¶…è¿‡å¯ç”¨æ ·æœ¬
        sample_indices = [idx for idx in sample_indices if idx < len(predictions)]
        num_samples = len(sample_indices)

        # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºå•ç‹¬çš„å›¾è¡¨
        for i, sample_idx in enumerate(sample_indices):
            plt.figure(figsize=(12, 4))

            pred = predictions[sample_idx]
            target = targets[sample_idx]

            # ç¡®ä¿æ‰€æœ‰æ•°æ®ç»´åº¦ä¸€è‡´
            x_vals = np.arange(len(target)) if len(times) != len(target) else times

            # ç»˜åˆ¶å®é™…å€¼
            plt.plot(x_vals, target, 'b-', label='actual value', linewidth=2)

            # ç»˜åˆ¶p50é¢„æµ‹å€¼ï¼ˆä¸­ä½æ•°ï¼‰
            plt.plot(x_vals, pred['p50'], 'r-', label='pred(p50)', linewidth=2)

            # åˆ›å»ºp10-p90çš„ç½®ä¿¡åŒºé—´
            plt.fill_between(x_vals, pred['p10'], pred['p90'],
                             color='r', alpha=0.2, label='10%-90% confidence')

            plt.title(f'Sample {sample_idx}: pred(p50) vs actual value')
            plt.xlabel('time step')
            plt.ylabel('value')
            plt.legend()
            plt.grid(True)

            # ä¿å­˜æ¯ä¸ªæ ·æœ¬çš„å›¾è¡¨
            plt.savefig(os.path.join(plot_dir, f'sample_{sample_idx}_prediction_epoch_{self.epoch}.png'))
            plt.close()

            # å°†å›¾ç‰‡æ·»åŠ åˆ°TensorBoard
            self.sw.add_figure(f'sample_{sample_idx}_prediction', plt.gcf(), self.epoch)

    def _plot_attention_maps(self, attention_components):
        """
        ç»˜åˆ¶æ³¨æ„åŠ›çƒ­å›¾

        Args:
            attention_components: åŒ…å«æ³¨æ„åŠ›æƒé‡çš„å­—å…¸
        """
        import matplotlib.pyplot as plt
        import os
        import numpy as np
        import seaborn as sns

        # ç¡®ä¿ä¿å­˜å›¾è¡¨çš„ç›®å½•å­˜åœ¨
        plot_dir = os.path.join(self.log_path, 'plots')
        if not os.path.exists(plot_dir):
            os.makedirs(plot_dir)

        # å¤„ç†ä¸åŒç±»å‹çš„æ³¨æ„åŠ›ç»„ä»¶
        if 'decoder_self_attn' in attention_components:
            # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„è‡ªæ³¨æ„åŠ›çŸ©é˜µ
            attn = attention_components['decoder_self_attn'][0].detach().cpu().numpy()

            # å¯¹äºå¤šå¤´æ³¨æ„åŠ›ï¼Œå–å¹³å‡å€¼
            if attn.ndim > 2:
                attn = np.mean(attn, axis=0)

            plt.figure(figsize=(10, 8))
            sns.heatmap(attn, cmap='viridis')
            plt.title('Decoder self-attention heat map')
            plt.xlabel('Key')
            plt.ylabel('value')

            # ä¿å­˜å›¾è¡¨
            plt.savefig(os.path.join(plot_dir, f'decoder_attention_epoch_{self.epoch}.png'))
            plt.close()

            # å°†å›¾ç‰‡æ·»åŠ åˆ°TensorBoard
            self.sw.add_figure('decoder_attention', plt.gcf(), self.epoch)

        # ç»˜åˆ¶å˜é‡é€‰æ‹©æƒé‡ - å¤„ç†å¤šç»´æƒé‡
        for weight_name in ['historical_flags', 'future_flags', 'static_flags']:
            if weight_name in attention_components:
                weights = attention_components[weight_name][0].detach().cpu().numpy()

                # æ£€æŸ¥æƒé‡çš„ç»´åº¦å¹¶é€‚å½“å¤„ç†
                print(f"{weight_name} weight_shape: {weights.shape}")

                if weights.ndim == 1:
                    # ä¸€ç»´æƒé‡ç›´æ¥ç»˜åˆ¶æ¡å½¢å›¾
                    plt.figure(figsize=(12, 4))
                    plt.bar(range(len(weights)), weights)
                    plt.title(f'{weight_name} Variable selection weight')
                    plt.xlabel('index')
                    plt.ylabel('weight')

                    # ä¿å­˜å›¾è¡¨
                    plt.savefig(os.path.join(plot_dir, f'{weight_name}_epoch_{self.epoch}.png'))
                    plt.close()

                    # å°†å›¾ç‰‡æ·»åŠ åˆ°TensorBoard
                    self.sw.add_figure(weight_name, plt.gcf(), self.epoch)

                elif weights.ndim == 2:
                    # äºŒç»´æƒé‡ç»˜åˆ¶çƒ­å›¾
                    plt.figure(figsize=(12, 8))
                    sns.heatmap(weights, cmap='viridis')
                    plt.title(f'{weight_name} Variable selection weight')

                    if weight_name == 'historical_flags':
                        plt.xlabel('feature')
                        plt.ylabel('time step')
                    elif weight_name == 'future_flags':
                        plt.xlabel('feature')
                        plt.ylabel('pred time step')

                    # ä¿å­˜å›¾è¡¨
                    plt.savefig(os.path.join(plot_dir, f'{weight_name}_heatmap_epoch_{self.epoch}.png'))
                    plt.close()

                    # å°†å›¾ç‰‡æ·»åŠ åˆ°TensorBoard
                    self.sw.add_figure(f'{weight_name}_heatmap', plt.gcf(), self.epoch)

                    # åŒæ—¶ç»˜åˆ¶æŒ‰æ—¶é—´æ­¥å¹³å‡çš„æƒé‡
                    mean_weights = np.mean(weights, axis=0)
                    plt.figure(figsize=(12, 4))
                    plt.bar(range(len(mean_weights)), mean_weights)
                    plt.title(f'{weight_name} Average variable selection weights')
                    plt.xlabel('feature index')
                    plt.ylabel('average weight')

                    # ä¿å­˜å›¾è¡¨
                    plt.savefig(os.path.join(plot_dir, f'{weight_name}_avg_epoch_{self.epoch}.png'))
                    plt.close()

                    # å°†å›¾ç‰‡æ·»åŠ åˆ°TensorBoard
                    self.sw.add_figure(f'{weight_name}_avg', plt.gcf(), self.epoch)

                    # ç»˜åˆ¶æ¯ä¸ªæ—¶é—´æ­¥çš„æƒé‡åˆ†å¸ƒ
                    plt.figure(figsize=(15, 10))
                    for i in range(min(weights.shape[0], 10)):  # æœ€å¤šæ˜¾ç¤º10ä¸ªæ—¶é—´æ­¥
                        plt.subplot(5, 2, i + 1)
                        plt.bar(range(weights.shape[1]), weights[i])
                        plt.title(f'time step {i}')
                        if i >= 8:  # åªåœ¨åº•éƒ¨ä¸¤ä¸ªå­å›¾æ˜¾ç¤ºxè½´æ ‡ç­¾
                            plt.xlabel('feature index')
                        plt.ylabel('weight')

                    plt.tight_layout()
                    # ä¿å­˜å›¾è¡¨
                    plt.savefig(os.path.join(plot_dir, f'{weight_name}_timesteps_epoch_{self.epoch}.png'))
                    plt.close()

                    # å°†å›¾ç‰‡æ·»åŠ åˆ°TensorBoard
                    self.sw.add_figure(f'{weight_name}_timesteps', plt.gcf(), self.epoch)

    def _plot_scatter(self, predictions, targets):
        """
        ç»˜åˆ¶é¢„æµ‹å€¼ä¸å®é™…å€¼çš„æ•£ç‚¹å›¾

        Args:
            predictions: æ‰€æœ‰é¢„æµ‹å€¼ï¼ˆ1Dæ•°ç»„ï¼‰
            targets: æ‰€æœ‰å®é™…å€¼ï¼ˆ1Dæ•°ç»„ï¼‰
        """
        import matplotlib.pyplot as plt
        import os
        import numpy as np
        from sklearn.metrics import r2_score

        # ç¡®ä¿ä¿å­˜å›¾è¡¨çš„ç›®å½•å­˜åœ¨
        plot_dir = os.path.join(self.log_path, 'plots')
        if not os.path.exists(plot_dir):
            os.makedirs(plot_dir)

        # è¿‡æ»¤æ‰NaNå’ŒInfå€¼
        valid_idx = ~(np.isnan(predictions) | np.isnan(targets) |
                      np.isinf(predictions) | np.isinf(targets))

        if np.sum(valid_idx) > 0:
            pred_filtered = predictions[valid_idx]
            target_filtered = targets[valid_idx]

            # è®¡ç®—ç›¸å…³æ€§ç³»æ•°å’ŒRÂ²
            corr = np.corrcoef(pred_filtered, target_filtered)[0, 1] if len(pred_filtered) > 1 else 0
            r2 = r2_score(target_filtered, pred_filtered)

            plt.figure(figsize=(10, 10))

            # ç»˜åˆ¶æ•£ç‚¹å›¾
            plt.scatter(target_filtered, pred_filtered, alpha=0.5)

            # æ·»åŠ å¯¹è§’çº¿ï¼ˆå®Œç¾é¢„æµ‹çº¿ï¼‰
            min_val = min(np.min(pred_filtered), np.min(target_filtered))
            max_val = max(np.max(pred_filtered), np.max(target_filtered))
            plt.plot([min_val, max_val], [min_val, max_val], 'r--')

            plt.title(f'pred vs actual (correlation coefficient: {corr:.4f}, RÂ²: {r2:.4f})')
            plt.xlabel('actual value')
            plt.ylabel('pred value')
            plt.grid(True)

            # ä¿å­˜å›¾è¡¨
            plt.savefig(os.path.join(plot_dir, f'scatter_plot_epoch_{self.epoch}.png'))
            plt.close()

            # å°†å›¾ç‰‡æ·»åŠ åˆ°TensorBoard
            self.sw.add_figure('scatter_plot', plt.gcf(), self.epoch)

    def _plot_error_distribution(self, predictions, targets):
        """
        ç»˜åˆ¶è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾

        Args:
            predictions: æ‰€æœ‰é¢„æµ‹å€¼åˆ—è¡¨
            targets: æ‰€æœ‰å®é™…å€¼åˆ—è¡¨
        """
        import matplotlib.pyplot as plt
        import os
        import numpy as np

        # ç¡®ä¿ä¿å­˜å›¾è¡¨çš„ç›®å½•å­˜åœ¨
        plot_dir = os.path.join(self.log_path, 'plots')
        if not os.path.exists(plot_dir):
            os.makedirs(plot_dir)

        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
        all_preds = np.concatenate(predictions) if predictions else np.array([])
        all_targets = np.concatenate(targets) if targets else np.array([])

        if len(all_preds) > 0 and len(all_targets) > 0:
            # è®¡ç®—ç»å¯¹è¯¯å·®å’Œç›¸å¯¹è¯¯å·®
            abs_errors = np.abs(all_preds - all_targets)

            # è¿‡æ»¤æ‰æ— æ•ˆå€¼
            valid_idx = ~(np.isnan(abs_errors) | np.isinf(abs_errors))
            if np.sum(valid_idx) > 0:
                filtered_errors = abs_errors[valid_idx]

                # ç»˜åˆ¶ç»å¯¹è¯¯å·®ç›´æ–¹å›¾
                plt.figure(figsize=(10, 6))
                plt.hist(filtered_errors, bins=50, alpha=0.75)
                plt.title('Absolute error distribution')
                plt.xlabel('Absolute error')
                plt.ylabel('freq')
                plt.grid(True)

                # ä¿å­˜å›¾è¡¨
                plt.savefig(os.path.join(plot_dir, f'abs_error_hist_epoch_{self.epoch}.png'))
                plt.close()

                # å°†å›¾ç‰‡æ·»åŠ åˆ°TensorBoard
                self.sw.add_figure('abs_error_hist', plt.gcf(), self.epoch)

                # è®¡ç®—å¹¶ç»˜åˆ¶ç›¸å¯¹è¯¯å·®ï¼ˆä»…å¯¹éé›¶ç›®æ ‡å€¼ï¼‰
                non_zero_targets = (all_targets != 0)
                if np.sum(non_zero_targets) > 0:
                    rel_errors = abs_errors[non_zero_targets] / np.abs(all_targets[non_zero_targets])

                    # è¿‡æ»¤æ‰æ— æ•ˆå€¼
                    valid_rel_idx = ~(np.isnan(rel_errors) | np.isinf(rel_errors))
                    if np.sum(valid_rel_idx) > 0:
                        filtered_rel_errors = rel_errors[valid_rel_idx]

                        # æˆªæ–­æç«¯å€¼ï¼Œé™åˆ¶åœ¨åˆç†èŒƒå›´å†…ï¼Œå¦‚0-2æˆ–æ›´çª„
                        filtered_rel_errors = np.clip(filtered_rel_errors, 0, 2)

                        plt.figure(figsize=(10, 6))
                        plt.hist(filtered_rel_errors, bins=50, alpha=0.75)
                        plt.title('Relative error distribution')
                        plt.xlabel('Relative error')
                        plt.ylabel('freq')
                        plt.grid(True)

                        # ä¿å­˜å›¾è¡¨
                        plt.savefig(os.path.join(plot_dir, f'rel_error_hist_epoch_{self.epoch}.png'))
                        plt.close()

                        # å°†å›¾ç‰‡æ·»åŠ åˆ°TensorBoard
                        self.sw.add_figure('rel_error_hist', plt.gcf(), self.epoch)

    def run(self):
        """
        start model training procedure (train > test > checkpoint > repeat)
        """
        for _ in range(self.epoch, self.cnf.epochs):
            self.train()

            with torch.no_grad():
                self.test()

            self.epoch += 1
            self.save_ck()

if __name__ == "__main__":
    from config.config import Config  # ç¡®ä¿ config é…ç½®æ–‡ä»¶è·¯å¾„æ­£ç¡®

    # cnf = Config()  # åˆå§‹åŒ–é…ç½®

    cnf = Config(conf_file_path=r"D:\PythonProject\chatbot\config\config\CESNET.yaml", exp_name="CESNET_100")
    cnf.device = "cuda"
    # cnf = Config(conf_file_path=r"D:\PythonProject\chatbot\config\config\synthetic_data.yaml", exp_name="synthetic_data")
    trainer = Trainer(cnf)  # åˆ›å»º Trainer å®ä¾‹
    trainer.run()  # è¿è¡Œè®­ç»ƒ
